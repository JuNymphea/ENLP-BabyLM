general:
    # Nickname for the whole run, set this to what you want!
    nickname: eval
    # model_name: finetune-nested-parens0.49_vocab500-uniform_seed8
    model_type: gpt2
    # checkpoint_number: 5000
    # checkpoint_number: 50
    # effective_batch_size describes ow big do we want the batches to be: per_device_train_batch_size * gradient_accumulation_steps. gradient accumulation steps are then set automatically given the per_device_train_batch_size in training.
    effective_batch_size: 128
    # How do we get the new embeddings matrix? Options: [leave_embeddings_alone, sample, use_pretrained_frozen]
    embeddings_strategy: sample
    # resume: true
    save_dir: checkpoints/finetune/gpt2_small
    wandb_project: ENLP

data:
    # Type and name are the two arguments we pass to huggingface datasets.load_dataset(data.type, data.name)
    # type:
    # name:
    # Tokenizer loaded with AutoTokenizer.from_pretrained(data.tokenizer_name, use_fast=True)
    tokenizer_name : "gpt2"
    # If general.embeddings_strategy is use_pretrained_frozen, which pretrained model should we take embeddings from. Otherwise can ommit this argument
    # embedding_model_name: 

dataset:
    id: "babylm_dataset/babylm.py"
    name: null
